<!DOCTYPE html>
<html lang="en-uk">
    <head>
         
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>Exact natural gradient in deep linear networks and application to the nonlinear case</title>
        <style>

    html, body {
        font-family: 'Alegreya Sans', sans-serif;
    }

    h1, h2, h3 {
        font-family: 'Alegreya', sans-serif;
    }

</style>


<link rel="stylesheet" href="https://hennequin-lab.github.io/css/main.css">


<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Alegreya">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Alegreya%20Sans">

<script defer src="https://hennequin-lab.github.io/css/friconix.js"></script>

<script defer>
 
function menu() {
  var x = document.getElementById("myTopnav");
  if (x.className === "topnav") {
    x.className = "topnav responsive slidedown";
  } else {
    x.className = "topnav";
  }
}

function menu_off() {
  var x = document.getElementById("myTopnav");
  x.className = "topnav";
}


window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  var topbutton = document.getElementById("topbutton");
  if (document.body.scrollTop > 50 || document.documentElement.scrollTop > 50) {
    topbutton.style.visibility = "visible";
    topbutton.style.color = "#666";
    topbutton.style.transform = "translateX(-50%)";
  } else {
    topbutton.style.visibility = "hidden";
    topbutton.style.color = "#fff";
    topbutton.style.transform = "translateX(-400%)";
  }
}


function topFunction() {
  document.body.scrollTop = 0; 
  document.documentElement.scrollTop = 0; 
}

</script> 

 <meta name="generator" content="Hugo 0.54.0" />
   </head>

    <body>
         

        <div class="topnav" id="myTopnav">
            <div class="topnavcontainer">
            <div class="topnavleft">
            
            
            
            <a href="/" class=""><span style="margin-right:0.3em;"><i class='fi-xnsuxl-home-solid'></i></span> HOME</a>
            
            <a href="/people/" class=""><span style="margin-right:0.3em;"><i class='fi-xnsuxl-team-solid'></i></span> PEOPLE</a>
            
            <a href="/publications/" class=""><span style="margin-right:0.3em;"><i class='fi-xwsuxl-brain-solid'></i></span> PUBLICATIONS</a>
            
            <a href="/contact/" class=""><span style="margin-right:0.3em;"><i class='fi-xnsuxl-envelope-solid'></i></span> CONTACT</a>
            
           
            </div>

            <div class="topnavright">
            
            
            <a href="https://scholar.google.co.uk/citations?user=-NkKYYcAAAAJ&amp;hl=en"><i class="fi-xnsxxl-graduation-cap-solid"></i></a>
            
            <a href="https://github.com/hennequin-lab/"><i class="fi-xnsxxl-github"></i></a>
            
            
            </div>
            <a href="javascript:void(0);" class="icon" onclick="menu()">
                <i class="fi-xnluxl-three-bars"></i>
            </a>
            </div>
        </div>

        <div class="topimg"></div>
        <div class="topbuttonbar"></div>
        <div class="topbuttonbar_transparent"></div>
        <div class="topbutton" onclick="topFunction()" id="topbutton"><i class="fi-ctlux2-chevron"></i></div>
        <div class="main" id="main" style="margin-top:3em;" onclick="menu_off()">



<div class="main">
    <div><a href=".." style="font-size: 200%; color: #aaaaaa;"><i class="fi-ctllxm-chevron"></i></a></div>


    <h4><a href="">Exact natural gradient in deep linear networks and application to the nonlinear case</a></h4>
    A Bernacchia, M Lengyel, and G Hennequin<br>
    <span style="color: #aa0000;">NeurIPS</span>, 2018 &nbsp;
    
     <a style="color:#000000; font-size:120%;" href="../bernacchia-nips-2018.pdf"><i class="fi-xnluxl-file-pdf-thin"></i></a> 
     <a style="color:#000000; font-size:120%;" href="https://github.com/ghennequin/bernacchia-nips-2018"><i class="fi-xnluxl-code"></i></a> 
 
    
    <br>
    <br>
    <h4>Abstract</h4>
    <br>
    <div class="text-justify"><p>Stochastic gradient descent (SGD) remains the method of choice for deep
learning, despite the limitations arising for ill-behaved objective functions.
In cases where it could be estimated, the <em>natural</em> gradient has proven very
effective at mitigating the catastrophic effects of pathological curvature in
the objective function, but little is known theoretically about its convergence
properties, and it has yet to find a practical implementation that would scale
to very deep and large networks.  Here, we derive an exact expression for the
natural gradient in deep linear networks, which exhibit pathological curvature
similar to the nonlinear case. We provide for the first time an analytical
solution for its convergence rate, showing that the loss decreases
exponentially to the global minimum in parameter space.  Our expression for the
natural gradient is surprisingly simple, computationally tractable, and
explains why some approximations proposed previously work well in practice.
This opens new avenues for approximating the natural gradient in the nonlinear
case, and we show in preliminary experiments that our online natural gradient
descent outperforms SGD on MNIST autoencoding while sharing its computational
simplicity.</p>

<p><div style="margin-top:3em; border: #000 solid 5px; position:relative;height:0;padding-bottom:74.98%"><iframe src="https://www.youtube.com/embed/G2aXqTNecO4?ecver=2" style="position:absolute;width:100%;height:100%;left:0" width="480" height="360" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div></p>
</div>
    

</div>

    </div> 

    <div class="friconix"><center><a href="http://friconix.com" style="color: #999999;">Icons by <i class="fi-lnsuxl-friconix"></i></a></center></div>
    </body>


</html>

